#install.packages("torch")
#install.packages("torchvision")
library(torch)
library(torchvision)
train_ds <- kmnist_dataset(
".",
download = TRUE,
train = TRUE,
transform = transform_to_tensor
)
test_ds <- kmnist_dataset(
".",
download = TRUE,
train = FALSE,
transform = transform_to_tensor
)
train_ds[1][[1]]$size()
train_dl <- dataloader(train_ds, batch_size = 20, shuffle = TRUE, drop_last=FALSE)
test_dl <- dataloader(test_ds, batch_size = 20, shuffle = TRUE, drop_last=FALSE)
train_iter <- train_dl$.iter()
train_iter$.next()
#print out the 1st batch of characters
par(mfrow = c(4,8), mar = rep(0, 4))
images <- train_dl$.iter()$.next()[[1]][1:32, 1, , ]
images %>%
purrr::array_tree(1) %>%
purrr::map(as.raster) %>%
purrr::iwalk(~{plot(.x)})
net <- nn_module(
"KMNIST-CNN",
initialize = function() {
# in_channels, out_channels, kernel_size, stride = 1, padding = 0
self$conv1 <- nn_conv2d(1, 32, 5)
self$conv2 <- nn_conv2d(32, 64, 5)
self$dropout1 <- nn_dropout2d(0.25)
self$fc1 <- nn_linear(64*10*10, 256)
self$fc2 <- nn_linear(256, 10)
},
forward = function(x) {
x %>%
self$conv1() %>%
nnf_relu() %>%
self$conv2() %>%
nnf_relu() %>%
nnf_max_pool2d(2) %>%
self$dropout1() %>%
torch_flatten(start_dim = 2) %>%
self$fc1() %>%
nnf_relu() %>%
self$fc2() %>%
nnf_sigmoid()
}
)
model<- net()
model <- net()
model$to(device = "cpu")
optimizer <- optim_adam(model$parameters)
for (epoch in 1:2) {
l <- c()
i <- 1
for (b in enumerate(test_dl)) {
# make sure each batch's gradient updates are calculated from a fresh start
optimizer$zero_grad()
# get model predictions
output <- model(b[[1]]$to(device = "cpu"))
# calculate loss
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cpu"))
# calculate gradient
loss$backward()
# apply weight updates
optimizer$step()
# track losses
l <- c(l, loss$item())
i <- i + 1
if(i %% 100 == 0){cat(paste("Loss at",mean(l),"-- i at:",i,"\n"))}
}
cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(l)))
}
model$eval()
test_losses <- c()
total <- 0
correct <- 0
count <- 0
for (b in enumerate(train_dl)) {
optimizer$zero_grad()
# get model predictions
output <- model(b[[1]]$to(device = "cpu"))
labels <- b[[2]]$to(device = "cpu")
# calculate loss
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
# add number of correct classifications in this batch to the aggregate
correct <- correct + as.numeric((predicted == labels)$sum())
count <- count + 1
if(count %% 100 == 0){cat(paste("Correct",correct,";count",count*20,"\n"))}
}
mean(test_losses)
correct/(count*20)
#library(devtools)
#install.packages("torch")
#install.packages("torchvision")
library(torch)
library(torchvision)
train_ds <- kmnist_dataset(
".",
download = TRUE,
train = TRUE,
transform = transform_to_tensor
)
test_ds <- kmnist_dataset(
".",
download = TRUE,
train = FALSE,
transform = transform_to_tensor
)
train_ds[1][[1]]$size()
train_dl <- dataloader(train_ds, batch_size = 20, shuffle = TRUE, drop_last=FALSE)
test_dl <- dataloader(test_ds, batch_size = 20, shuffle = TRUE, drop_last=FALSE)
train_iter <- train_dl$.iter()
train_iter$.next()
#print out the 1st batch of characters
par(mfrow = c(4,8), mar = rep(0, 4))
images <- train_dl$.iter()$.next()[[1]][1:32, 1, , ]
images %>%
purrr::array_tree(1) %>%
purrr::map(as.raster) %>%
purrr::iwalk(~{plot(.x)})
net <- nn_module(
"KMNIST-CNN",
initialize = function() {
# in_channels, out_channels, kernel_size, stride = 1, padding = 0
self$conv1 <- nn_conv2d(1, 32, 5)
self$conv2 <- nn_conv2d(32, 64, 5)
self$dropout1 <- nn_dropout2d(0.25)
self$fc1 <- nn_linear(64*10*10, 256)
self$fc2 <- nn_linear(256, 10)
},
forward = function(x) {
x %>%
self$conv1() %>%
nnf_relu() %>%
self$conv2() %>%
nnf_relu() %>%
nnf_max_pool2d(2) %>%
self$dropout1() %>%
torch_flatten(start_dim = 2) %>%
self$fc1() %>%
nnf_relu() %>%
self$fc2() %>%
nnf_sigmoid()
}
)
model<- net()
#dimension of input tensor
x <- torch_randn(c(1, 1, 28, 28))
conv1 <- nn_conv2d(1, 32, 5)
conv1(x)$size()
conv2 <- nn_conv2d(32, 64, 5)
conv2(conv1(x))$size()
maxpool1 <- nn_max_pool2d(2)
maxpool1(conv2(conv1(x)))$size()
torch_flatten(maxpool1(conv2(conv1(x))), start_dim = 2)$size()
linear1 <- nn_linear(64*10*10, 256)
linear1(torch_flatten(maxpool1(conv2(conv1(x))), start_dim = 2))$size()
linear2 <- nn_linear(256, 10)
linear2(linear1(torch_flatten(maxpool1(conv2(conv1(x))), start_dim = 2)))$size()
model <- net()
model$to(device = "cuda")
model <- net()
model$to(device = "cuda")
optimizer <- optim_adam(model$parameters)
for (epoch in 1:2) {
l <- c()
i <- 1
for (b in enumerate(train_dl)) {
# make sure each batch's gradient updates are calculated from a fresh start
optimizer$zero_grad()
# get model predictions
output <- model(b[[1]]$to(device = "cpu"))
# calculate loss
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cpu"))
# calculate gradient
loss$backward()
# apply weight updates
optimizer$step()
# track losses
l <- c(l, loss$item())
i <- i + 1
if(i %% 100 == 0){cat(paste("Loss at",mean(l),"-- i at:",i,"\n"))}
}
cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(l)))
}
model <- net()
model$to(device = "cuda")
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
model <- net()
model$to(device = "cuda")
optimizer <- optim_adam(model$parameters)
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
optimizer <- optim_adam(model$parameters)
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
model <- net()
model$to(device = "cuda")
optimizer <- optim_adam(model$parameters)
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
model <- net()
model$to(device = "cuda")
optimizer <- optim_adam(model$parameters)
for (epoch in 1:2) {
l <- c()
i <- 1
for (b in enumerate(train_dl)) {
# make sure each batch's gradient updates are calculated from a fresh start
optimizer$zero_grad()
# get model predictions
output <- model(b[[1]]$to(device = "cpu"))
# calculate loss
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
# calculate gradient
loss$backward()
# apply weight updates
optimizer$step()
# track losses
l <- c(l, loss$item())
i <- i + 1
if(i %% 100 == 0){cat(paste("Loss at",mean(l),"-- i at:",i,"\n"))}
}
cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(l)))
}
model <- net()
model$to(device = "cuda")
optimizer <- optim_adam(model$parameters)
for (epoch in 1:2) {
l <- c()
i <- 1
for (b in enumerate(train_dl)) {
# make sure each batch's gradient updates are calculated from a fresh start
optimizer$zero_grad()
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
# calculate loss
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
# calculate gradient
loss$backward()
# apply weight updates
optimizer$step()
# track losses
l <- c(l, loss$item())
i <- i + 1
if(i %% 100 == 0){cat(paste("Loss at",mean(l),"-- i at:",i,"\n"))}
}
cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(l)))
}
model <- net()
model$to(device = "cuda")
optimizer <- optim_adam(model$parameters)
for (epoch in 1:50) {
l <- c()
i <- 1
for (b in enumerate(train_dl)) {
# make sure each batch's gradient updates are calculated from a fresh start
optimizer$zero_grad()
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
# calculate loss
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
# calculate gradient
loss$backward()
# apply weight updates
optimizer$step()
# track losses
l <- c(l, loss$item())
i <- i + 1
if(i %% 100 == 0){cat(paste("Loss at",mean(l),"-- i at:",i,"\n"))}
}
cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(l)))
}
model <- net()
#library(devtools)
#install.packages("torch")
#install.packages("torchvision")
library(torch)
library(torchvision)
model <- net()
model$to(device = "cuda")
optimizer <- optim_adam(model$parameters)
for (epoch in 1:2) {
l <- c()
i <- 1
for (b in enumerate(train_dl)) {
# make sure each batch's gradient updates are calculated from a fresh start
optimizer$zero_grad()
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
# calculate loss
loss <- nnf_cross_entropy(output, b[[2]]$to(device = "cuda"))
# calculate gradient
loss$backward()
# apply weight updates
optimizer$step()
# track losses
l <- c(l, loss$item())
i <- i + 1
if(i %% 100 == 0){cat(paste("Loss at",mean(l),"-- i at:",i,"\n"))}
}
cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(l)))
}
model$eval()
test_losses <- c()
total <- 0
correct <- 0
count <- 0
for (b in enumerate(test_dl)) {
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
labels <- b[[2]]$to(device = "cuda")
# calculate loss
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
# add number of correct classifications in this batch to the aggregate
correct <- correct + as.numeric((predicted == labels)$sum())
count <- count + 1
if(count %% 100 == 0){cat(paste("Correct",correct,";count",count*20,"\n"))}
}
test_losses <- c()
total <- 0
correct <- 0
count <- 0
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
labels <- b[[2]]$to(device = "cuda")
# calculate loss
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
# add number of correct classifications in this batch to the aggregate
correct <- correct + as.numeric((predicted == labels)$sum())
count <- count + 1
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
predicted
labels
predicted == labels
as.numeric((predicted == labels)$sum())
predicted == labels
sum(predicted == labels)
IntTensor.item(sum(predicted == labels))
tensor_int()
torch_int(labels)
x <- torch_int()
x
x$to_list()
as_array(x)
torch_tensor(x)
x$tolist()
x$to_list()
x$to_list
(predicted == labels)$to_list
labels$to_list
predicted == labels
x <- predicted == labels
x
x$sum
x$`_values`
x$sum()
x$sum()$to_list()
x$sum()$to_list
x$values
x$values()
x$trace
x$trace()
x$type_as()
x$type_as("int")
x$type_as(int)
to_list()x
to_list(x)
x$to_list()]
x$to_list()
x$to_list
x$to_int
x$to_dense
x$to_dense()
x$to
x$to()
x$`_version`
x$`_version`()
x$sum()
x <- x$sum()
x
as.array(x)
x$to(device = "cpu")
x
x.argmax()
xargmax()
x$argmax()
x$argmax()$to(devive="cpu")
x$to(device="cpu")
as.numeric((predicted == labels)$sum()$to(device = "cuda"))
as.numeric((predicted == labels)$sum()$to(device = "cpu"))
test_losses <- c()
total <- 0
correct <- 0
count <- 0
for (b in enumerate(test_dl)) {
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
labels <- b[[2]]$to(device = "cuda")
# calculate loss
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
# add number of correct classifications in this batch to the aggregate
correct <- correct + as.numeric((predicted == labels)$sum()$to(device = "cpu"))
count <- count + 1
if(count %% 100 == 0){cat(paste("Correct",correct,";count",count*20,"\n"))}
}
mean(test_losses)
correct/(count*20)
test_losses <- c()
total <- 0
correct <- 0
count <- 0
for (b in enumerate(test_dl)) {
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
labels <- b[[2]]$to(device = "cuda")
# calculate loss
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
# add number of correct classifications in this batch to the aggregate
correct <- correct + as.numeric((predicted == labels)$sum()$to(device = "cpu"))
count <- count + 1
if(count %% 100 == 0){cat(paste("Correct",correct,";count",count*20,"\n"))}
}
mean(test_losses)
correct/(count*20)
test_losses <- c()
total <- 0
correct <- 0
count <- 0
for (b in enumerate(test_dl)) {
# get model predictions
output <- model(b[[1]]$to(device = "cuda"))
labels <- b[[2]]$to(device = "cuda")
# calculate loss
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
# add number of correct classifications in this batch to the aggregate
correct <- correct + as.numeric((predicted == labels)$sum()$to(device = "cpu"))
count <- count + 1
if(count %% 100 == 0){cat(paste("Correct",correct,";count",count*20,"\n"))}
}
mean(test_losses)
correct/(count*20)
knit_with_parameters('~/GitHub/Torch-For-R-CNN-Example/Torch-for-R-CNN-Example.Rmd')
unlink('Torch-for-R-CNN-Example_cache', recursive = TRUE)
net <- nn_module(
"KMNIST-CNN",
initialize = function() {
# in_channels, out_channels, kernel_size, stride = 1, padding = 0
self$conv1 <- nn_conv2d(1, 8, 5)
self$conv2 <- nn_conv2d(8, 16, 5)
self$dropout1 <- nn_dropout2d(0.25)
self$fc1 <- nn_linear(16*10*10, 256)
self$fc2 <- nn_linear(256, 10)
},
forward = function(x) {
x %>%
self$conv1() %>%
nnf_relu() %>%
self$conv2() %>%
nnf_relu() %>%
nnf_max_pool2d(2) %>%
self$dropout1() %>%
torch_flatten(start_dim = 2) %>%
self$fc1() %>%
nnf_relu() %>%
self$fc2() %>%
nnf_sigmoid()
}
)
model<- net()
#dimension of input tensor
x <- torch_randn(c(1, 1, 28, 28))
conv1 <- nn_conv2d(1, 32, 5)
conv1(x)$size()
conv2 <- nn_conv2d(32, 64, 5)
conv2(conv1(x))$size()
maxpool1 <- nn_max_pool2d(2)
maxpool1(conv2(conv1(x)))$size()
torch_flatten(maxpool1(conv2(conv1(x))), start_dim = 2)$size()
linear1 <- nn_linear(64*10*10, 256)
linear1(torch_flatten(maxpool1(conv2(conv1(x))), start_dim = 2))$size()
linear2 <- nn_linear(256, 10)
linear2(linear1(torch_flatten(maxpool1(conv2(conv1(x))), start_dim = 2)))$size()
